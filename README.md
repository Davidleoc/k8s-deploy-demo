## Docker + Kubernetes + Self-Hosted Runner

Este projeto demonstra um pipeline completo de CI/CD usando GitHub Actions, Docker Hub e Kubernetes, com foco em ambientes de rede interna. Para a demonstraÃ§Ã£o foi usado um projeto simples em Go (feito por IA)

 ### O objetivo principal Ã© automatizar todo o fluxo:

Fazer build da aplicaÃ§Ã£o (Ã© nescessÃ¡rio rodar ./run.sh localmente no diretÃ³rio ~/actions-runner)

Criar a imagem Docker

Publicar a imagem no Docker Hub

Atualizar o Deployment no Kubernetes automaticamente

#### SoluÃ§Ã£o: Self-Hosted Runner

O Github Actions nÃ£o acessa servidores em rede interna. Para permitir que o pipeline acesse o cluster, configurei um Self-Hosted GitHub Runner dentro do servidor Ubuntu 22.04 que contem um cluster kubernetes feito com kaind.
Assim, ele consegue:

âœ” Executar kubectl localmente
âœ” Fazer o build da imagem
âœ” Enviar para o Docker Hub
âœ” Atualizar o Deployment no cluster

ğŸ“ Onde instalei o runner:

/home/github/actions-runner

ğŸ“ PermissÃµes necessÃ¡rias

Eu precisei copiar o kubeconfig para o usuÃ¡rio do runner:

sudo mkdir -p /home/github/.kube

sudo cp /root/.kube/config /home/github/.kube/config

sudo chown -R github:github /home/github/.kube

### ğŸ”§ Arquitetura do Projeto

k8s-deploy-demo/

â”‚

â”œâ”€â”€ app/

â”‚ â”œâ”€â”€ main.go

â”‚ â””â”€â”€ Dockerfile

â”‚

â”œâ”€â”€ k8s/

â”‚ â”œâ”€â”€ deployment.yaml

â”‚ â””â”€â”€ service.yaml

â”‚

â””â”€â”€ .github/

â””â”€â”€ workflows/

â””â”€â”€ deploy.yaml

### ğŸ³ Docker

A imagem da aplicaÃ§Ã£o Ã© construÃ­da usando o Dockerfile dentro da pasta /app:

FROM golang:1.20-alpine
WORKDIR /app
COPY . .
RUN go build -o server
EXPOSE 8080
CMD ["./server"]


Eu faÃ§o push da imagem no meu Docker Hub com:

davidl05/k8s-demo:v1

### â˜¸ï¸ Kubernetes

Eu utilizo um cluster Kubernetes local, rodando em servidores Linux dentro da minha rede interna.
Esse cluster foi criado usando kind (Kubernetes in Docker), a ideia foi fazer um kluster minimo para validar conceitos.

ğŸ–¥ï¸ Nodes

Um node Ã© um servidor (ou container, no caso do kind) que executa workloads do Kubernetes.

No meu ambiente atual, tenho:

1 node atuando como control-plane

Nenhum node dedicado como worker (o prÃ³prio control-plane tambÃ©m executa os pods de aplicaÃ§Ã£o)

A saÃ­da real do comando:

kubectl get nodes

Mostra:

NAME                     STATUS   ROLES           AGE     VERSION
k8s-demo-control-plane   Ready    control-plane   5h14m   v1.30.0


Ou seja: tudo â€” API Server, Scheduler, Controller Manager e atÃ© minha aplicaÃ§Ã£o â€” roda no mesmo nÃ³.

### ğŸ“¦ Pods

Aqui estÃ¡ a lista real dos pods que estÃ£o rodando no meu cluster:

kubectl get pods -A

NAMESPACE            NAME                                             READY   STATUS    RESTARTS        AGE

default              demo-deployment-66c55f56c7-6n882                 1/1     Running   0               4h42m

default              demo-deployment-66c55f56c7-f27hm                 1/1     Running   0               4h42m

default              demo-deployment-66c55f56c7-tcfb4                 1/1     Running   0               4h43m

ingress-nginx        ingress-nginx-controller-6775c6fd56-snwqn        1/1     Running   1 (4h55m ago)   5h13m

kube-system          coredns-7db6d8ff4d-2xf6s                         1/1     Running   1 (4h55m ago)   5h14m

kube-system          coredns-7db6d8ff4d-nkzsf                         1/1     Running   1 (4h55m ago)   5h14m

kube-system          etcd-k8s-demo-control-plane                      1/1     Running   1 (4h55m ago)   5h14m

kube-system          kindnet-b829w                                    1/1     Running   1 (4h55m ago)   5h14m

kube-system          kube-apiserver-k8s-demo-control-plane            1/1     Running   1 (4h55m ago)   5h14m

kube-system          kube-controller-manager-k8s-demo-control-plane   1/1     Running   1 (4h55m ago)   5h14m

kube-system          kube-proxy-wzxjk                                 1/1     Running   1 (4h55m ago)   5h14m

kube-system          kube-scheduler-k8s-demo-control-plane            1/1     Running   1 (4h55m ago)   5h14m

local-path-storage   local-path-provisioner-988d74bc-9q4p6            1/1     Running   2 (4h54m ago)   5h14m


InformaÃ§Ãµes importantes:

âœ” 3 rÃ©plicas da minha aplicaÃ§Ã£o (demo-deployment)
âœ” Ingress NGINX funcionando corretamente
âœ” Componentes core do cluster (coredns, etcd, apiserver, etc) estÃ£o estÃ¡veis
âœ” Armazenamento padrÃ£o do kind (local-path-storage)

### ğŸ“– Manifests Kubernetes usados

A aplicaÃ§Ã£o Ã© descrita com manifestos YAML:

deployment.yaml

3 rÃ©plicas

imagem atualizada automaticamente pelo pipeline

estratÃ©gia rolling update

service.yaml

Service do tipo ClusterIP para a aplicaÃ§Ã£o

ingress.yaml

expÃµe a aplicaÃ§Ã£o usando o ingress-nginx

permite acessar via URL interna

### ğŸŒ Como ocorre o deploy

Eu faÃ§o um git push origin main

O GitHub dispara o workflow

O self-hosted runner pega o cÃ³digo

Faz build da imagem Docker

Envia para o Docker Hub

Usa kubectl set image para atualizar o Deployment

O Kubernetes inicia o update

As novas rÃ©plicas entram no ar sem downtime

### âœ… Resultado final

CI/CD real funcionando

Deploy automÃ¡tico no meu cluster Kubernetes local

Pipeline rodando via self-hosted runner

AtualizaÃ§Ã£o contÃ­nua da imagem Docker

update sem interrupÃ§Ã£o
